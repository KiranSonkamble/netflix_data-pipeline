{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8607ce3d-2791-40a3-b057-4304e9cf53f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3064594-3bda-4b3c-a5fe-35b8638c9891",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import necessary libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, to_date,mean , avg, floor\n",
    "from pyspark.sql.types import IntegerType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e471056f-c1c7-40af-927a-ebd0a1549ace",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Data from (ADLS)"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"abfss://bronze@storageforproject.dfs.core.windows.net/netflix_titles/*\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca9a799-88bb-4874-a06a-97d6255bb3ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check for Null Values in DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# check the nulls in dataframe\n",
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc72291a-2d0a-4eab-b352-784c0c9115ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "chek the schema"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2bce4e-2e95-44ea-8095-b2df78d271f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "drop unwanted column"
    }
   },
   "outputs": [],
   "source": [
    "# The _rescued_data column in your DataFrame is created by Databricks Auto Loader when it encounters data that does not conform to the expected schema.\n",
    "df = df.drop('_rescued_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbba2642-8aa1-45b6-a79c-167c90c9e4d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fill the col duration_min"
    }
   },
   "outputs": [],
   "source": [
    "# change the datatype of duration_minutes from string to integer\n",
    "df = df.withColumn(\"duration_minutes\", col(\"duration_minutes\").cast(IntegerType()))\n",
    "\n",
    "# check the average and mean of the duration_min column\n",
    "duration_min_stats = df.select(mean(\"duration_minutes\").alias(\"mean_duration\"), avg(\"duration_minutes\").alias(\"avg_duration\"))\n",
    "duration_min_stats.display()\n",
    "\n",
    "# \n",
    "mean_duration = df.select(mean('duration_minutes')).collect()[0][0]\n",
    "# fill na\n",
    "df = df.fillna(mean_duration, subset=['duration_minutes'])\n",
    "\n",
    "# display\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94379e3d-528f-4316-ac18-18448972f7e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fillna to col duration seasons"
    }
   },
   "outputs": [],
   "source": [
    "# check the uniqueness in the column\n",
    "df.groupBy('duration_seasons').count().orderBy(col('count').desc()).display()\n",
    "# change the value where greater than 13\n",
    "df = df.withColumn(\"duration_seasons\", when(col(\"duration_seasons\") > 13, 13).otherwise(col(\"duration_seasons\")))\n",
    "\n",
    "# change the dataype of col\n",
    "df= df.withColumn(\"duration_seasons\",col(\"duration_seasons\").cast(IntegerType()))\n",
    "\n",
    "# fill the null value with mean\n",
    "duration_seasons_mean = df.select(floor(mean(\"duration_seasons\"))).collect()[0][0]\n",
    " \n",
    "df= df.fillna(duration_seasons_mean,subset=[\"duration_seasons\"])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4069267-d332-46d9-ba1b-f1ddcc4dabe3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fill col rating with most frequent value"
    }
   },
   "outputs": [],
   "source": [
    "# get most frequent rating\n",
    "most_freq_rating = df.groupBy(\"rating\").count().orderBy(col(\"count\").desc()).first()[0]\n",
    "\n",
    "# fill na with the most frequent rating\n",
    "df = df.fillna(most_freq_rating,subset=[\"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a3af04-8090-4d0f-b2ba-8226ae699152",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fill with most appropriate value"
    }
   },
   "outputs": [],
   "source": [
    "# get most frequent title and type\n",
    "most_freq_type = df.groupBy(\"type\").count().orderBy(col(\"count\").desc()).first()[0]\n",
    "\n",
    "# fill na with the most frequent title and type\n",
    "df = df.fillna({\"title\": \"No title\", \"type\": most_freq_type})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6f081e-c561-4475-b992-94a500e9fe31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "this part is followed in delta live table"
    }
   },
   "outputs": [],
   "source": [
    "# # change datype to int\n",
    "# df = df.withColumn(\"show_id\", col('show_id').cast(IntegerType()))\n",
    "# most_frequent_released_yr = df.groupBy(col('release_year').cast(StringType())).count().orderBy(col('count').desc()).first()[0]\n",
    "\n",
    "# # fill na \n",
    "# df = df.fillna({\"show_id\":1000, \"description\": \"No description\", \"release_year\":most_frequent_released_yr, \"date_added\":\"01/01/2000\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1041fbd3-d9a6-4dac-a867-c1b02961d797",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "recheck the null values"
    }
   },
   "outputs": [],
   "source": [
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f512f79-68bd-4f4c-8095-c46885b5d300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write to delta table\n",
    "\n",
    "df.write.format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"path\", \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_titles\")\\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_transform title data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
