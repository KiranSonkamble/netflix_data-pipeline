{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afba8e79-d895-4194-9963-74b65deba356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create streaming dataframe that reads data from source increamently.\n",
    "\n",
    "Setting up streaming query that uses autoloader to moniter a cloud storage for new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da2037d-a79e-47c8-bef4-222d214d947b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create a streaming data frame"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_location= \"abfss://checkpoints@storageformetastore.dfs.core.windows.net/\"\n",
    "\n",
    "df= spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\",checkpoint_location)\\\n",
    "    .load(\"abfss://raw@storageforproject.dfs.core.windows.net/\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7dffec-7a84-4d94-8a99-a5ce3a1ed26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write stream method allows you to continuously append data to the target starage as new files arrive.\n",
    "\n",
    "Checkpoint ensures that the write operations is fault-tolerant, if the stream pipeline fails. It can resume from where it left off without duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3417458c-6e0f-40cf-98e2-271a67d48eb9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write/ store files in target location"
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .start(\"abfss://bronze@storageforproject.dfs.core.windows.net/netflix_titles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a2c2ff-c541-43ec-93fb-f6fc9cc628b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check if File stored or not"
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"abfss://bronze@storageforproject.dfs.core.windows.net/netflix_titles/\")\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cbb0df-f8ac-4d38-b8ca-250cd2c373b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`cloudFiles` is used because Auto Loader simplifies the ingestion process, especially for continuously arriving data. It automatically detects new files, tracks processed ones, and handles schema evolution. If we used `readStream` without specifying `cloudFiles`, we would have to handle these aspects manually, which is less efficient and more error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41db5b8-1e09-48f3-8cd8-78bddf4cb3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7887043691301590,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Increamental data loading using Autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
