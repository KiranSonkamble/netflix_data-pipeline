{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afba8e79-d895-4194-9963-74b65deba356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create streaming pipeline that reads data from source increamently.\n",
    "Setting up streaming query that uses autoloader to moniter a cloud storage for new files\n",
    "\n",
    "**Below cell creates a streaming DataFrame df by reading CSV files from the specified path in the Azure Data Lake Storage (ADLS) Raw container. It uses the cloudFiles format for auto-loading and specifies the schema location for schema inference and evolution. The checkpoint_location is used to store the schema information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da2037d-a79e-47c8-bef4-222d214d947b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "create a streaming data frame"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_location= \"abfss://checkpoints@storageformetastore.dfs.core.windows.net/\"\n",
    "\n",
    "df= spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\",\"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\",checkpoint_location)\\\n",
    "    .load(\"abfss://raw@storageforproject.dfs.core.windows.net/\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7dffec-7a84-4d94-8a99-a5ce3a1ed26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write stream method allows you to continuously append data to the target starage as new files arrive.\n",
    "\n",
    "Checkpoint ensures that the write operations is fault-tolerant, if the stream pipeline fails. It can resume from where it left off without duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3417458c-6e0f-40cf-98e2-271a67d48eb9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write/ store files in target location"
    }
   },
   "outputs": [],
   "source": [
    "df.writeStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .start(\"abfss://bronze@storageforproject.dfs.core.windows.net/netflix_titles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7468c60c-da58-4611-8082-a0cb08d788c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**above cell writes the streaming DataFrame df to the specified path in the ADLS Bronze container. It uses the csv format and appends new data to the existing files. The checkpoint_location is used to store the checkpoint information, ensuring fault tolerance and exactly-once processing. The stream is triggered to process data every 10 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a2c2ff-c541-43ec-93fb-f6fc9cc628b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read File stored"
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"abfss://bronze@storageforproject.dfs.core.windows.net/netflix_titles/\")\n",
    "\n",
    "#  reads the CSV files from the specified path in the ADLS Bronze container and stores the result in the DataFrame df_read\n",
    "\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cbb0df-f8ac-4d38-b8ca-250cd2c373b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`cloudFiles` is used because Auto Loader simplifies the ingestion process, especially for continuously arriving data. It automatically detects new files, tracks processed ones, and handles schema evolution. If we used `readStream` without specifying `cloudFiles`, we would have to handle these aspects manually, which is less efficient and more error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6207a608-1489-4711-ad37-20ee0d4ffc95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The notebook's overall purpose is to demonstrate a streaming data pipeline that reads cloud files, processes the data, and stores the results in a target location within Azure Data Lake Storage. The use of checkpoints ensures fault tolerance and data integrity in the streaming process. The notebook provides a structured approach to handling streaming data in an Azure Databricks environment using Spark."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7887043691301590,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Increamental data loading using Autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
