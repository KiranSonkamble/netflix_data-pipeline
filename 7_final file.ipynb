{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f1d3db-3637-41b1-8cde-15945cea93ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## to Gold Layer\n",
    "Delta Live Tables (DLT) transformations for reading, transforming, and saving data from Delta files stored in Azure Data Lake Storage (ADLS) Gen2. The purpose of each cell is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4043789d-dfd8-4514-b3ea-b68808370f5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "libraries"
    }
   },
   "outputs": [],
   "source": [
    "#  imports necessary libraries and functions for use in the subsequent cells.\n",
    "import dlt\n",
    "from pyspark.sql.functions import col, when, count\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "362e9f9b-28fa-45c9-8788-e759e2489c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths for one table (Netflix cast)\n",
    "silver_path_netflix_cast = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_cast\"\n",
    "\n",
    "@dlt.table(comment=\"Read Delta files from silver container (batch)\")\n",
    "def netflix_cast():\n",
    "    return spark.read.format(\"delta\").load(silver_path_netflix_cast)\n",
    "\n",
    "@dlt.table(comment=\"Transforms the data by removing duplicate rows\")\n",
    "@dlt.expect(\"valid_show_id\", \"show_id IS NOT NULL\")\n",
    "def netflix_cast_transformed():\n",
    "    df = dlt.read(\"netflix_cast\").dropDuplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b92b4c-005e-4d69-bb82-f04068daf7fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and Transform Netflix Category Data"
    }
   },
   "outputs": [],
   "source": [
    "silver_path_netflix_category = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_category\"\n",
    "\n",
    "\n",
    "@dlt.table(comment=\"Read Delta files from silver container (batch)\")\n",
    "def netflix_category():\n",
    "    df = spark.read.format(\"delta\").load(silver_path_netflix_category)\n",
    "    return df\n",
    "\n",
    "@dlt.table(comment=\"Transforms the data by removing duplicate rows and ensures that the show_id column is not null. \")\n",
    "@dlt.expect(\"valid_show_id\", \"show_id IS NOT NULL\")\n",
    "\n",
    "def netflix_category_transformed():\n",
    "    return(\n",
    "        dlt.read(\"netflix_category\").dropDuplicates()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2706690-4659-42ba-950c-966a1dc13b08",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read netflix_countries"
    }
   },
   "outputs": [],
   "source": [
    "silver_path_netflix_countries = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_countries\"\n",
    "\n",
    "@dlt.table(comment=\"Read netflix_countries files from silver container (batch)\")\n",
    "\n",
    "def netflix_countries():\n",
    "    return (spark.read.format(\"delta\").load(silver_path_netflix_countries))\n",
    "\n",
    "@dlt.table(comment=\"Transforms the data by removing duplicate rows and ensures that the show_id column is not null. \")\n",
    "@dlt.expect(\"valid_show_id\", \"show_id IS NOT NULL\")\n",
    "\n",
    "def netflix_countries_transformed():\n",
    "\n",
    "    return(\n",
    "        dlt.read(\"netflix_countries\").dropDuplicates()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e8aaf3-dc00-40e9-92dd-6957dfe67e0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read netflix_directors"
    }
   },
   "outputs": [],
   "source": [
    "silver_path_netflix_directors = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_directors\"\n",
    "\n",
    "@dlt.table(comment=\"Read netflix_directors files from silver container (batch)\")\n",
    "\n",
    "def netflix_directors():\n",
    "    return(spark.read.format(\"delta\").load(silver_path_netflix_directors))\n",
    "\n",
    "@dlt.table(comment=\"Transforms the data by removing duplicate rows and ensures that the show_id column is not null. \")\n",
    "@dlt.expect(\"valid_show_id\", \"show_id IS NOT NULL\")\n",
    "\n",
    "def netflix_directors_transformed():\n",
    "    return(\n",
    "        dlt.read(\"netflix_directors\").dropDuplicates()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97eda8af-d41a-485c-8cab-c0d1e96ae50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the Silver data is static and only updated occasionally, `batch` reading makes sense. Batch reads are straightforward and efficient for one-time processing. However, if the Silver layer is continuously being updated with new data (like in a real-time pipeline), `streaming` would be better to process those updates incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23acac12-2a37-4bf7-a406-22c9690d48a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Streaming netflix_titles table"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "silver_path_netflix_titles = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_titles\"\n",
    "\n",
    "@dlt.table(comment=\"read netflix_titles files from silver container(stream)\")\n",
    "\n",
    "def stream_netflix_titles():\n",
    "      return (\n",
    "            spark.readStream.format(\"delta\").load(silver_path_netflix_titles)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c4a41c-f146-4421-830d-c0db65b499fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "validate data"
    }
   },
   "outputs": [],
   "source": [
    "# DLT expectations are for validating data, not transforming it.\n",
    "# EXPECTATION RULES (DATA QUALITY CHECKS)\n",
    "expectation_rules = {\n",
    "    \"valid_show_id\": \"show_id IS NOT NULL\",          # Drop rows with null show_id\n",
    "    \"valid_release_year\": \"release_year IS NOT NULL\",# Drop rows with null release_year\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d14c22-a003-4600-91a5-14e3d0bf3144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"cleaned netflix_titles table \")\n",
    "@dlt.expect_all_or_drop(expectation_rules)\n",
    "\n",
    "def clean_netflix_titles():\n",
    "    df = dlt.read(\"stream_netflix_titles\").dropDuplicates()\\\n",
    "        .withColumn(\"description\", when(col(\"description\").isNull(), \"No description\").otherwise(col(\"description\")))\\\n",
    "        .withColumn(\"release_year\",col(\"release_year\").cast(IntegerType()))\\\n",
    "        .withColumn(\"new_flag\",when(col(\"type\")==\"Movie\",1).otherwise(0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9d8447-3531-4d38-bad3-1b08ec053206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "when reading from a Delta table that's part of the same pipeline, using dlt.read is better for lineage and dependency management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b7dc191-f001-49cd-b103-4ff35ecefdb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(comment=\"create a view\")\n",
    "@dlt.expect(\"valid_release_year\",\"release_year IS NOT NULL\")\n",
    "\n",
    "def netflix_title_view():\n",
    "    return(\n",
    "        dlt.read(\"clean_netflix_titles\")\n",
    "        .withColumn(\"new_flag\",when(col(\"type\")==\"Movie\",1).otherwise(0))\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f832a3-d431-4e29-b3ee-1c91589b27fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"final gold table\")\n",
    "@dlt.expect(\"valid_show_id\",\"show_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid date_added\",\"date_added IS NOT NULL\")\n",
    "\n",
    "\n",
    "def netflix_titles_gold():\n",
    "    return(\n",
    "        dlt.read(\"clean_netflix_titles\").dropDuplicates()\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7_final file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
