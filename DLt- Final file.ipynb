{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f1d3db-3637-41b1-8cde-15945cea93ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4043789d-dfd8-4514-b3ea-b68808370f5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "libraries"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192c1ad2-a4fc-4bc6-8827-3969b95a7b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://silver@storageforproject.dfs.core.windows.net/netflix_cast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "606a6f56-4532-477d-b66e-ea04fade8dc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Batch read netflix_cast  table"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"Read Delta files from silver container (batch)\")\n",
    "def netflix_cast():\n",
    "    \n",
    "    return (spark.read.format(\"delta\")\n",
    "            .load(silver_path)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97eda8af-d41a-485c-8cab-c0d1e96ae50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the Silver data is static and only updated occasionally, `batch` reading makes sense. Batch reads are straightforward and efficient for one-time processing. However, if the Silver layer is continuously being updated with new data (like in a real-time pipeline), `streaming` would be better to process those updates incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23acac12-2a37-4bf7-a406-22c9690d48a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream read netflix_cast table"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(comment=\"Stream Delta files from silver container\")\n",
    "def netflix_casts():\n",
    "      \n",
    "    df = (spark.readStream.format(\"delta\")\n",
    "          .load(silver_path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b92b4c-005e-4d69-bb82-f04068daf7fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read table netflix_category"
    }
   },
   "outputs": [],
   "source": [
    "# @dlt.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe9d8447-3531-4d38-bad3-1b08ec053206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "when reading from a Delta table that's part of the same pipeline, using dlt.read is better for lineage and dependency management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c4a41c-f146-4421-830d-c0db65b499fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "validate data"
    }
   },
   "outputs": [],
   "source": [
    "# DLT expectations are for validating data, not transforming it.\n",
    "# EXPECTATION RULES (DATA QUALITY CHECKS)\n",
    "expectation_rules = {\n",
    "    \"valid_show_id\": \"show_id IS NOT NULL\",          # Drop rows with null show_id\n",
    "    \"valid_release_year\": \"release_year IS NOT NULL\",# Drop rows with null release_year\n",
    "    \"valid_date_added\": \"date_added IS NOT NULL\"     # Drop rows with null date_added\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLt- Final file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
